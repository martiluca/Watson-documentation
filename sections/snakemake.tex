\newpage
\part{Snakemake}

This Snakemake pipeline is designed to perform bioinformatics analyses on genomic data, specifically focusing on variant calling, quality control, and copy number variation analysis from DNA sequencing data (FASTQ files). The pipeline is modular, allowing for easy addition or modification of steps as needed.

The pipeline is composed of several rules, each defining a step in the analysis process. Each rule will be explained in detail, including the commands executed and the purpose of each step.

Each rule contains a logging parameter that specifies where to write log outputs, allowing for easy troubleshooting and monitoring of the pipeline's execution.

\section{Rules}
\subsection{Alignment}
This rule is responsible for aligning paired-end FASTQ files, sorting and indexing the resulting BAM file, and removing duplicates.

\begin{lstlisting}[breaklines=true, language=bash]
    bwa mem -t 12 -R '@RG\\tID:sampleID\\tSM:sampleID' REF sample.fastq.gz | bamsort inputformat=sam markduplicates=1 rmdup=1 fixmates=1 inputthreads=8 outputthreads=8 M=sample_metrics.txt index=1 O=sample.bam indexfilename=sample.bai;
    touch {output.done};
\end{lstlisting}

\subsubsection*{Step 1}

\begin{lstlisting}[breaklines=true, language=bash]
    bwa mem -t 12 -R '@RG\\tID:sampleID\\tSM:sampleID' REF sample.fastq.gz
\end{lstlisting}

This command runs the BWA MEM algorithm, which aligns the paired-end FASTQ files to the reference genome. The options have the following functions:
\begin{itemize}
    \item \code{-t} specifies the number of threads to use.
    \item \code{-R} provides the read group information for the BAM file. It includes metadata that helps downstream tools (e.g., GATK) differentiate between data from different samples or sequencing runs. In this case, it specifies the read group ID and sample name.
\end{itemize}

\subsubsection*{Step 2}

\begin{lstlisting}[breaklines=true, language=bash]
    bamsort inputformat=sam markduplicates=1 rmdup=1 fixmates=1 inputthreads=8 outputthreads=8 M=sample_metrics.txt index=1 O=sample.bam indexfilename=sample.bai
\end{lstlisting}

This command sorts the aligned reads, marks duplicates, removes duplicates, and fixes mate-pair information. Here are what the different parameters do:
\begin{itemize}
    \item \code{inputformat=sam} sort the input SAM file (the output of \texttt{bwa mem}).
    \item \code{markduplicates=1} mark the duplicate reads that result from PCR amplification during sequencing.
    \item \code{rmdup=1} remove the duplicate reads.
    \item \code{fixmates=1} fix the mate-pair information in the BAM file.
    \item \code{inputthreads=8} use 8 threads for input processing.
    \item \code{outputthreads=8} use 8 threads for output processing.
    \item \code{M=sample\_metrics.txt} save the metrics of the sorting process in a file.
    \item \code{index=1} create an index file for the BAM file.
    \item \code{O=sample.bam} save the sorted BAM file.
    \item \code{indexfilename=sample.bai} specify the name of the index file.
\end{itemize}

\subsubsection*{Step 3}

\begin{lstlisting}[breaklines=true, language=bash]
    touch {output.done}
\end{lstlisting}

This command creates an empty \texttt{output.done} file to signal that the alignment rule has completed successfully. This file is used for tracking progress and directing the workflow. In the following rules, the explanation for this command will be omitted.

\subsection{Alignment QC}

This rule generates alignment statistics from BAM files using \texttt{Samtools stats} and processes these stats with a custom Python script.

\begin{lstlisting}[breaklines=true, language=bash]
    samtools stats sample.bam > sample_samtools_stats.txt;
    path/to/calc_align_qc.py OUTPUT_DIR sample_samtools_stats.txt;
\end{lstlisting}

\subsubsection*{Step 1}

\begin{lstlisting}[breaklines=true, language=bash]
    samtools stats sample.bam > sample_samtools_stats.txt;
\end{lstlisting}

This command generates alignment statistics from the BAM file. It provides information such as the number of reads mapped, coverage depth, and error rates.

\subsubsection*{Step 2}

\begin{lstlisting}[breaklines=true, language=bash]
    path/to/calc_align_qc.py OUTPUT_DIR sample_samtools_stats.txt;
\end{lstlisting}

This command runs a custom Python script that calculates alignment quality control metrics. The script takes two arguments: the output directory and the alignment statistics file. The script reads the alignment statistics, computes additional metrics, and writes the results to a TSV file.

The additional metrics computed include total unique sequences (in megabases), percentage of unmapped reads, percentage of discordant pairs, percentage of non-reference bases and percentage of duplicate reads.

\subsection{On Target Calculation}
\subsubsection{Generate on target calculation files}

This rule processes each sample's BAM file to generate on-target statistics and depth of coverage information for specific regions (SNPs, coding, and others).

\begin{lstlisting}[breaklines=true, language=bash]
    samtools view -F 1024 -L path/to/CORE_covered_regions.bed -u sample.bam | samtools stats > sample_onTarget_samtools_stats.txt;
    samtools depth -b path/to/CORE_ROI_SNP.bed -o sample_SNP_depth.tmp sample.bam; 
    samtools depth -b path/to/CORE_ROI_CODING.bed -o sample_CODING_depth.tmp sample.bam;
    samtools depth -b path/to/CORE_ROI_OTHER.bed -o sample_OTHER_depth.tmp sample.bam;
\end{lstlisting}

\subsubsection*{Step 1}

\begin{lstlisting}[breaklines=true, language=bash]
    samtools view -F 1024 -L path/to/CORE_covered_regions.bed -u sample.bam | samtools stats > sample_onTarget_samtools_stats.txt;
\end{lstlisting}

\paragraph*{samtools view.} Filters the BAM file by excluding duplicate reads. The flag \code{1024} indicates duplicate reads; only unique reads are processed. The \code{-L} option lmits the reads to only those that overlap the regions defined in the baitset BED file (\texttt{CORE\_covered\_regions.bed }). 

\paragraph*{samtools stats.} Takes the output from the previous command (filtered BAM) and calculates alignment statistics (e.g., number of reads aligned to the target regions, average depth, ...). The output is saved in \texttt{sample\_onTarget\_samtools\_stats.txt}.

\subsubsection*{Step 2 (SNPs)}

\begin{lstlisting}[breaklines=true, language=bash]
    samtools depth -b path/to/CORE_ROI_SNP.bed -o sample_SNP_depth.tmp sample.bam; 
\end{lstlisting}

This command calculates the depth of coverage (number of reads covering each base) for specific regions in the BAM file. The \code{-b} option specifies the BED file with the regions of interest (SNPs), and the output is saved in \texttt{sample\_SNP\_depth.tmp}.

\subsubsection*{Step 3 (coding regions)}

\begin{lstlisting}[breaklines=true, language=bash]
    samtools depth -b path/to/CORE_ROI_CODING.bed -o sample_CODING_depth.tmp sample.bam; 
\end{lstlisting}

This command calculates the depth of coverage for specific regions in the BAM file. The \code{-b} option specifies the BED file with the regions of interest (coding regions), and the output is saved in \texttt{sample\_CODING\_depth.tmp}.

\subsubsection*{Step 4 (others)}

\begin{lstlisting}[breaklines=true, language=bash]
    samtools depth -b path/to/CORE_ROI_OTHER.bed -o sample_OTHER_depth.tmp sample.bam;
\end{lstlisting}

This command calculates the depth of coverage for specific regions in the BAM file. The \code{-b} option specifies the BED file with the regions of interest (other regions), and the output is saved in \texttt{sample\_OTHER\_depth.tmp}.

\subsubsection{On target calculation}

This rule calculates the on-target metrics and average depth for each sample using a custom Python script and cleans up temporary files that were generated in the previous step.

\begin{lstlisting}[breaklines=true, language=bash]
    path/to/calc_onTarget_qc.py OUTPUT_DIR sample_onTarget_samtools_stats.txt sample.bam path/to/CORE_covered_regions.bed STATIC_DIR;
    touch {output.done_3};
    rm -f OUTPUT_DIR/sample_*_depth.tmp;
\end{lstlisting}

\subsubsection*{Step 1}
\begin{lstlisting}[breaklines=true, language=bash]
    path/to/calc_onTarget_qc.py OUTPUT_DIR sample_onTarget_samtools_stats.txt sample.bam path/to/CORE_covered_regions.bed STATIC_DIR;
\end{lstlisting}

This command runs the \texttt{calc\_onTarget\_qc.py} script that calculates various QC metrics related to the coverage and on-target performance for sequencing data for each sample.

It reads the samtools stats file and extracts key quality control parameters (e.g., total sequences, average read length) into a dictionary. And the performs the following actions:
\begin{itemize}
    \item Compute the total size of the target regions (in base pairs) from the baitset BED file.
    \item Compute the average depths for specific regions of interest (SNPs, coding regions, and other regions) by reading depth information from temporary depth files.
    \item Compute several metrics:
    \begin{itemize}
        \item On-target megabases: total number of bases aligned to the target regions.
        \item Design size: total size of the target regions, converted to megabases.
        \item Average depth: average depth of coverage across all target regions.
        \item Average depth in specific regions (SNP, coding, other): average depth of coverage in each region.
        \item On-target percentage: percentage of reads aligned to the target regions.
    \end{itemize}
\end{itemize}

\subsubsection*{Step 2}

\begin{lstlisting}[breaklines=true, language=bash]
    rm -f OUTPUT_DIR/sample_*_depth.tmp;
\end{lstlisting}

This command removes the temporary depth files that were created in the previous steps for calculating depth in specific regions.

\subsection{Contamination Index Calculation}

This rule calculates the contamination index, which helps assess the level of contamination in the sample by looking at specific SNPs that can indicate contamination when the observed allele frequencies deviate from expectations.

\begin{lstlisting}[breaklines=true, language=bash]
    path/to/calc_ContaminationIndex.py path/to/CORE_cont_check_SNPs.bed OUTPUT_DIR sample.bam SAMPLE_INFO;
    touch {output.done_4};
    rm -f sample_tmp_xy.bed sample_tmp_aut.bed;
\end{lstlisting}

\subsubsection*{Step 1}

\begin{lstlisting}[breaklines=true, language=bash]
    path/to/calc_ContaminationIndex.py path/to/CORE_cont_check_SNPs.bed OUTPUT_DIR sample.bam SAMPLE_INFO;
\end{lstlisting}

This command runs the \texttt{calc\_ContaminationIndex.py} script that estimates the contamination index for each analyzed sample using sequencing data. It creates temporary BED files for the XY chromosomes and autosomes by splitting the baitset file and retrieves gender and sample-related information from the sample information file.

Contamination calculation:
\begin{itemize}
    \item \textbf{Y-based contamination.} For samples with female or unknown gender, the script counts the reads from chromosome Y and autosomes using samtools to detect contamination. It normalizes the read counts and calculates the percentage of contamination using predefined means and standard deviations.
    \item \textbf{X-based contamination.} For male samples, the script generates a pileup of SNPs on chromosome X and compares observed alleles against expected alleles from the baitset. It calculates the fraction of non-genotype alleles to estimate the contamination level.
\end{itemize}

If there is insufficient depth (too few reads), the script records that the contamination result is “Unknown - insufficient depth”.\\

As a final step, the script appends the contamination percentages to a results tsv file. It also appends sample metadata such as gender, amount of DNA used, and library concentration.

\subsubsection*{Step 2}

\begin{lstlisting}[breaklines=true, language=bash]
    rm -f sample_tmp_xy.bed sample_tmp_aut.bed;
\end{lstlisting}

This command removes the temporary files used during the contamination index calculation.

\subsection{Clipping}

This rule processes BAM files by clipping reads (hard clipping), filtering out unwanted reads (those not in proper pairs, failing quality checks, or duplicates), and generates a new BAM file and its corresponding index.

\begin{lstlisting}[breaklines=true, language=bash]
    samtools view -@ 4 -u -f 2 -F 3840  sample.bam | java -jar path/to/ClipBamOverlap.jar --fasta REF --clipMode HARD --coordSort --output sample_clip.bam;
    samtools index sample_clip.bam;
\end{lstlisting}

\subsubsection*{Step 1}

\begin{lstlisting}[breaklines=true, language=bash]
    samtools view -@ 4 -u -f 2 -F 3840  sample.bam | java -jar path/to/ClipBamOverlap.jar --fasta REF --clipMode HARD --coordSort --output sample_clip.bam
\end{lstlisting}

\paragraph*{samtools view.} The option \code{-u} outputs the data in uncompressed BAM format to speed up processing, the option \code{-f 2} filters out reads that are not in proper pairs, and the option \code{-F 3840} filters out reads that are duplicates, fail quality checks, or are secondary alignments. The option \code{-@ 4} specifies the number of threads to use.

\paragraph*{java.} This command runs the clipping tool. he clipping mode is set to "HARD" clipping, which means the bases outside the alignment region are removed completely rather than being soft clipped (which would retain them in the sequence but mark them as clipped).

\subsubsection*{Step 2}

\begin{lstlisting}[breaklines=true, language=bash]
    samtools index sample_clip.bam;
\end{lstlisting}

After clipping, this command indexes the new clipped BAM file.

\subsection{Variant Calling}
This is the part of the workflow responsible for calling genetic variants using Mutect2. It takes as input BAM files and outputs a VCF file containing the called variants and uses a panel of normals (pon) for filtering.

\begin{lstlisting}[breaklines=true, language=bash]
    path/to/java -jar path/to/gatk Mutect2 -R REF -I sample_clip.bam -O sample.vcf.gz --panel-of-normals pon.vcf.gz --native-pair-hmm-threads 12
\end{lstlisting}

\subsection{Variant Filtering}

\begin{lstlisting}[breaklines=true, language=bash]
    zless [input.vcf] | grep -v ';PON'| bcftools query - f "%CHROM\\t%POS\\t[%AD\\t%DP\\n]" | sed 's!,!\\t!' | awk '$5>=20 && $4>=5 {{print $1"\\t"$2}}' | gzip > sample_filtered.vcf.gz
\end{lstlisting}

\subsubsection*{Step 1}

\begin{lstlisting}[breaklines=true, language=bash]
    bcftools query - f "%CHROM\\t%POS\\t[%AD\\t%DP\\n]"
\end{lstlisting}

\paragraph{bcftools query.} Uses bcftools, a tool for the manipulation of VCF files; it allows to extract specific information about variants. \code{-f} specifies the output format:

\begin{itemize}
    \item \textbf{\%CHROM}: chromosome;
    \item \textbf{\%POS}: position on the chromosome;
    \item \textbf{\%AD}: allele depth (ref and alt);
    \item \textbf{\%DP}: depth of coverage.
\end{itemize}

The output is a table with comma-separated columns containing the above information.

\paragraph{sed.} Used to substitute the comma with a tab in the output of bcftools query.

\paragraph{awk.} Filters the output of the previous commands, selecting only the rows where the depth of coverage is greater than or equal to 20 and the allele depth of the alternative allele is greater than or equal to 5. The output is a table with two columns (chromosome and position) containing the selected variants.

\subsubsection*{Step 2}
\begin{lstlisting}[breaklines=true, language=bash]
    bcftools view -R [sample_filtered_ID.vcf.gz] [input.vcf] > sample_filtered.vcf.gz
\end{lstlisting}

\code{-R} specifies an input file containing a list of regions (chromosome and position) to be extracted from the compressed VCF file (\texttt{sample\_filtered\_ID.vcf.gz}). The variants in the input file (\texttt{input.vcf}) are filtered based on the regions in the list. The result is saved in \texttt{sample\_filtered.vcf.gz}.

\subsubsection*{Step 3}
\begin{lstlisting}[breaklines=true, language=bash]
    bcftools norm -m -both [sample_filtered.vcf.gz] | ~/PROGS/vt/vt decompose_blocksub - -o [sample_decom.vcf]
\end{lstlisting}

This command combines bcftools and vt to normalize and decompose the variants in the input file (\texttt{sample\_filtered.vcf.gz}). The output is saved in \texttt{sample\_decom.vcf}. The goal is to transform complex variants in simpler ones and to decompose them for a more detailed analysis.

\paragraph*{bcftools norm.} Normalizes the variants in the input file. 

\begin{itemize}
    \item \code{-m} merges multiallelic variants into a single record.
    \item \code{-both} decomposes both multiallelic (variants with more than one alternative allele) and complex variants (multi-nucleotide polymorphisms, MNPs) in simpler variants. 
\end{itemize}

The final result is a file in wich the complex variants are separated in single variants for each allele.

\paragraph*{Vt.} A tool for variant normalization and decomposition. \code{decompose\_blocksub} specifies for the decomposition of “block substitutions”, which are variants in which multiple adjacent nucleotides are substituted simultaneously. This command subtitutes those variants with a series of single nucleotide substitutions.

\subsubsection*{Step 4}
\begin{lstlisting}[breaklines=true, language=bash]
    bcftools view --types snps [sample_decom.vcf] | bgzip > [sample_snps.vcf.gz]
\end{lstlisting}

\code{--types snps} filters the variants in the input file (\texttt{sample\_decom.vcf} obtained in step 3) selecting only the single nucleotide polymorphisms (SNPs). 

The output is saved in \texttt{sample\_snps.vcf.gz}.

\subsubsection*{Step 5}
\begin{lstlisting}[breaklines=true, language=bash]
    bcftools index --tbi [sample_snps.vcf.gz]
\end{lstlisting}

Creates a TBI (Tabix Index) file for the compressed VCF file \texttt{sample\_snps.vcf.gz}.

\subsubsection*{Step 6}
\begin{lstlisting}[breaklines=true]
    bcftools view --types indels [sample_decom.vcf] | bcftools norm --fasta-ref [REF] --output-type z --output [sample_indel_filt_norm.vcf.gz]
\end{lstlisting}

\paragraph*{bcftools view.} Filters the variants in the input file (\texttt{sample\_decom.vcf} obtained in step 3) selecting only the indels.

\paragraph*{bcftools norm.} Normalizes the indels in the input file using a fasta reference (\code{--fasta-ref [REF]}). Normalizing variants means to align them correctly with the reference sequence, removing redundancy and representing them in a standard way.

\subsubsection*{Steps 7, 8, 9.} 
Basically the same as steps 4, 5, 6.

\subsubsection*{Step 10}
\begin{lstlisting}[breaklines=true]
    bcftools view --types snps [sample_decom_CN.vcf] | bgzip > sample_decom_CN.vcf.gz
\end{lstlisting}

Filters the variants in the input file (\texttt{sample\_decom\_CN.vcf} obtained in step 9) selecting only the SNPs. The output is saved in \texttt{sample\_decom\_CN.vcf.gz}.

\subsection{SV Calling (Pindel)}

\subsubsection{Prepare Pindel BAM}

The rule generates a BAM file for Pindel, the tool used for structural variant detection, by filtering out supplementary alignment reads and creating an index for the processed BAM file.

\begin{lstlisting}[breaklines=true, language=bash]
    samtools view -@ 4 -b -F 2048 -o sample_pindel.bam sample.bam;
    samtools index sample_pindel.bam;
\end{lstlisting}

\subsubsection*{Step 1}

\begin{lstlisting}[breaklines=true, language=bash]
    samtools view -@ 4 -b -F 2048 -o sample_pindel.bam sample.bam;
\end{lstlisting}

Samtools view filters out supplementary alignment reads from the input BAM file (\texttt{sample.bam}) and saves the result in a new BAM file (\texttt{sample\_pindel.bam}). The option \code{-F 2048} filters out supplementary alignment reads.

\subsubsection*{Step 2}

\begin{lstlisting}[breaklines=true, language=bash]
    samtools index sample_pindel.bam;
\end{lstlisting}

This command creates an index for the filtered BAM file.

\subsubsection{Prepare Pindel Config}

This rule generates a configuration file for Pindel by combining the sample-specific BAM file with metadata like the average insert size and the reference normal BAM file.

\begin{lstlisting}[breaklines=true, language=python]
    with open(params.sample_qc_file, 'r') as stream:
        for line in stream:
            s = line.strip()
            sp = s.split('\t')
            if sp[0] == 'AVERAGE_INSERT_SIZE_BP':
                sample_ins = round(float(sp[1]))
    with open(output.config_pindel, 'a') as stream2:
        stream2.write( str(input.bam_pindel) + '\t' + str(sample_ins) + '\t' + str(params.sampleID) + '\n' + str(params.normal_bam) + '\t150\tNormal')	
\end{lstlisting}

\subsubsection{Pindel}

This rule is responsible for detecting structural variants (SVs) in a sample's genome using Pindel, generating and filtering the resulting VCF file, and then realigning indels. 

\begin{lstlisting}[breaklines=true, language=bash]
    pindel -T 10 --fasta REF --config-file sample_pindel_config.txt --output-prefix sample_SV --anchor_quality 10 --include path/to/CORE_covered_regions.bed --minimum_support_for_event 20 --name_of_logfile log_dir/011-SVCalling_rule_sample.log;
    pindel2vcf --reference REF --reference_name GRCh38 --reference_date 20160222 --pindel_output_root sample_SV;
    bgzip sample_SV.vcf;
    bcftools index --tbi sample_SV.vcf.gz;
    bcftools norm --fasta-ref REF sample_SV.vcf.gz --output sample_SV_norm.vcf;
    python3 path/to/filter_SV.py sample_SV_norm.vcf sample_SV_norm_filt.vcf;
    bgzip sample_SV_norm_filt.vcf;
    bcftools index --tbi sample_SV_norm_filt.vcf.gz;
    bcftools view sample_SV_norm_filt.vcf.gz --samples sampleID -O z -o sample_SV_filt_fin_sample.vcf.gz;
    bcftools index --tbi sample_SV_filt_fin_sample.vcf.gz;
\end{lstlisting}

\subsubsection*{Step 1}

\begin{lstlisting}[breaklines=true, language=bash]
    pindel -T 10 --fasta REF --config-file sample_pindel_config.txt --output-prefix sample_SV --anchor_quality 10 --include path/to/CORE_covered_regions.bed --minimum_support_for_event 20 --name_of_logfile log_dir/011-SVCalling_rule_sample.log;
\end{lstlisting}

This command runs Pindel. An event will be considered only if it has an anchor quality score of 10, is in the specific region of interest (baitset BED file), and has a minimum support of 20 reads. 

\subsubsection*{Step 2}

\begin{lstlisting}[breaklines=true, language=bash]
    pindel2vcf --reference REF --reference_name GRCh38 --reference_date 20160222 --pindel_output_root sample_SV;
\end{lstlisting}

This command converts the output from Pindel to a VCF file format using the reference genome GRCh38.

\subsubsection*{Step 3}

\begin{lstlisting}[breaklines=true, language=bash]
    bgzip sample_SV.vcf;
\end{lstlisting}

This compresses the generated VCF file using \texttt{bgzip} to produce a \texttt{.gz} file.

\subsubsection*{Step 4}

\begin{lstlisting}[breaklines=true, language=bash]
    bcftools index --tbi sample_SV.vcf.gz;
\end{lstlisting}

This command creates a TBI (Tabix Index) file for the compressed VCF file \texttt{sample\_SV.vcf.gz}.

\subsubsection*{Step 5}

\begin{lstlisting}[breaklines=true, language=bash]
    bcftools norm --fasta-ref REF sample_SV.vcf.gz --output sample_SV_norm.vcf;
\end{lstlisting}

This command normalizes the variants in the input file (\texttt{sample\_SV.vcf.gz}) using the reference genome. Normalizing variants means to align them correctly with the reference sequence, removing redundancy and representing them in a standard way.

\subsubsection*{Step 6}

\begin{lstlisting}[breaklines=true, language=bash]
    python3 path/to/filter_SV.py sample_SV_norm.vcf sample_SV_norm_filt.vcf;
\end{lstlisting}

This command runs a Python script that filters the variants. For each variant (non-header line), it checks the genotype (GT) and allele depth (AD) for both the normal sample and the current sample.  It only keeps variants where the normal sample has a genotype of 0/0 (no variant). It looks for variants in the sample that are not 0/0 and calculates the allele depth (AD) based on the genotype (0/1, 1/1, or 1/.).
It only retains variants where the sample's allele depth (AD) is greater than 20.

\subsubsection*{Step 7}

\begin{lstlisting}[breaklines=true, language=bash]
    bgzip sample_SV_norm_filt.vcf;
\end{lstlisting}

This compresses the filtered VCF file using \texttt{bgzip} to produce a \texttt{.gz} file.

\subsubsection*{Step 8}

\begin{lstlisting}[breaklines=true, language=bash]
    bcftools index --tbi sample_SV_norm_filt.vcf.gz;
\end{lstlisting}

This command creates a TBI (Tabix Index) file for the compressed VCF file \\ \texttt{sample\_SV\_norm\_filt.vcf.gz}.

\subsubsection*{Step 9}

\begin{lstlisting}[breaklines=true, language=bash]
    bcftools view sample_SV_norm_filt.vcf.gz --samples sampleID -O z -o sample_SV_filt_fin_sample.vcf.gz;
\end{lstlisting}

This command filters the variants in the input file (\texttt{sample\_SV\_norm\_filt.vcf.gz}) to keep only the variants for the sample of interest (\texttt{sampleID}). 

\subsubsection*{Step 10}

\begin{lstlisting}[breaklines=true, language=bash]
    bcftools index --tbi sample_SV_filt_fin_sample.vcf.gz;
\end{lstlisting}

This command creates a TBI (Tabix Index) file for the compressed VCF file\\ \texttt{sample\_SV\_filt\_fin\_sample.vcf.gz}.

\subsection{BRASS}

This rule uses the tool Brass for structural variant (SV) calling, focusing on rearrangements supported by at least 10 reads, and annotates them with additional feature information.

The \code{run} directive is used instead of \code{shell} the sample quality control file is generated during the pipeline execution (i.e., it does not exist when launching the pipeline).

\begin{lstlisting}[breaklines=true, language=python]
    sample_bam = OUTPUT_DIR + '/' + params.sampleID + '/' + params.sampleID + '_clip.bam'
    sample_qc = OUTPUT_DIR + '/' + params.sampleID + '/' + params.sampleID + '_qc.tsv'
    normal_qc = os.path.join(STATIC_DIR, 'Normals_watson', 'normal_pool_qc.txt')
    maxInsSize = getMaxInsertSizeForBrass(sample_qc, normal_qc)
    cmd1 = "brass-group -q 10 -n 4 -F " + params.sv_repeats + " -I " + params.sv_noncontigs + " -m " + str(maxInsSize) + " " + sample_bam + " " + params.normal_bam + " >" + params.brass_out
    cmd2 = "grep -v \'#\' " + params.brass_out + " | awk \'$29>=10 && $9+$10+$11+$12+$13+$14+$15+$16+$17+$18+$19+$20+$21+$22+$23+$24+$25+$26+$27+$28==0 {print $0}\' >" + params.brass_out_filt
    cmd3 = params.py_annot_brass + " " + params.brass_out_filt + " " + params.gene_footprints + " " + output.brass_final
    print(cmd1)
    subprocess.call(cmd1, shell=True)
    print(cmd2)
    subprocess.call(cmd2, shell=True)
    print(cmd3)
    subprocess.call(cmd3, shell=True)
\end{lstlisting}

\subsubsection*{Step 1}

\begin{lstlisting}[breaklines=true, language=python]
    maxInsSize = getMaxInsertSizeForBrass(sample_qc, normal_qc)
\end{lstlisting}

The rule first computes the maximum insert size for Brass, by calling a custom function that reads quality control files from both the sample and the normal control.

\subsubsection*{Step 2}

\begin{lstlisting}[breaklines=true, language=python]
    cmd1 = "brass-group -q 10 -n 4 -F " + params.sv_repeats + " -I " + params.sv_noncontigs + " -m " + str(maxInsSize) + " " + sample_bam + " " + params.normal_bam + " >" + params.brass_out
\end{lstlisting}

The first command runs Brass (brass-group) with the following options:
\begin{itemize}
    \item \code{-q 10} sets the quality threshold for supporting reads to 10;
    \item \code{-n 4} specifies the number of threads to use;
    \item \code{-F} and \code{-I} specify the repeat regions and non-contiguous regions to exclude from the analysis;
    \item \code{-m} sets the maximum insert size;
\end{itemize}

The sample's BAM file and the normal BAM file are used as inputs to detect rearrangements.

\subsubsection*{Step 3}

\begin{lstlisting}[breaklines=true, language=python]
    cmd2 = "grep -v \'#\' " + params.brass_out + " | awk \'$29>=10 && $9+$10+$11+$12+$13+$14+$15+$16+$17+$18+$19+$20+$21+$22+$23+$24+$25+$26+$27+$28==0 {print $0}\' >" + params.brass_out_filt
\end{lstlisting}

The second command filters the Brass output to keep only rearrangements supported by at least 10 reads (column 29) and with no evidence of other types of rearrangements.

\subsubsection*{Step 4}

\begin{lstlisting}[breaklines=true, language=python]
    cmd3 = params.py_annot_brass + " " + params.brass_out_filt + " " + params.gene_footprints + " " + output.brass_final
\end{lstlisting}

The third command runs a Python script that extracts genomic information about breakpoints from the Brass output. For each breakpoint (A and B), it checks if the genomic coordinates overlap with any genes in the gene\_footprint.bed file. If genes are found, it annotates the rearrangement with the gene names. If not, it annotates with a “\texttt{.}” indicating no overlap.

\subsection{Copy Number (PureCN)}

This rule identifies copy number variations (CNVs) in a tumor sample by processing coverage data from BAM files, normalizing it for GC content, and applying a segmentation algorithm to detect CNVs. This rule also annotates detected CNVs with gene information, including the cytoband location.

The code is lengthy and complex, so we will not provide a detailed explanation here. The rule involves several steps:

\begin{enumerate}
    \item \lstinline|Rscript $PURECN/Coverage.R|. This R script calculates GC-normalized coverage from the tumor's BAM file using predefined genomic intervals (\texttt{CNwindows\_hg38.txt}) and generates the coverage file (\texttt{sample\_cov\_loess}). 
    \item \lstinline|SnpSift.jar annotate dbsnp.vcf.gz|. This Java command annotates the decomposed VCF file with known SNPs from the \texttt{dbsnp.vcf.gz} file, creating \texttt{vcf\_snps\_CN\_annot}.
    \item \lstinline|PureCN.R|. This R script runs the PureCN algorithm to detect CNVs using the coverage data and the annotated VCF. It applies the “betabin” model and a segmentation algorithm (PSCBS) to identify CNVs and further optimizes the results by factoring in variant allele frequencies.
    \item Several awk and grep commands are used to process the CNV gene list:
        \begin{itemize}
            \item The CSV file (\texttt{CN\_genes}) is filtered and formatted to include key columns such as gene symbol, chromosome, start/end positions, copy number, focality, and LOH (loss of heterozygosity).
            \item The script then checks the CNV list against a core gene list (\texttt{CORE\_genes\_list.txt}) and selects only relevant genes for further analysis.
        \end{itemize}
    \item \lstinline|cytobands_tracks.py|. This script is run to annotate the final CNV gene list with cytoband information from the \texttt{cytobands\_hg38.bed} file.
    \item \lstinline|add_purity_to_QC.py|. This script adjusts the QC report with tumor purity information derived from the copy number analysis.
\end{enumerate}

\subsection{Variant Annotation}

This rule is used annotate and filter variants. As for the previous rule, the code is long and complex, so we will not provide a detailed explanation here. The rule involves several steps:

\begin{enumerate}
    \item VEP Annotation. VEP (Variant Effect Predictor) is run on the input VCFs to annotate each variant. It uses several options to include information such as existing variations, gnomAD allele frequencies, and protein impact predictions (SIFT, PolyPhen).
    \item SnpSift Annotation. After VEP annotation, the VCF is further annotated with dbSNP using SnpSift. This step integrates known dbSNP variants into the VCF.
    \item Filtering. An intermediate annotation file is created by filtering variants for significance. Filters are applied to exclude benign/tolerated variants, low-impact variants, or variants in certain regions like HLA. Variants passing these filters are written to the output. For example, variants with SIFT scores of “benign” or “tolerated” are excluded. Only variants with moderate to high impact or splicing effects are kept. Structural variants undergo a similar process, but the filtering criteria are adjusted for larger changes like structural rearrangements.
    \item Final annotation. The final annotation and filtering are handled by \code{annotateFinalFilt\_new.py}, which integrates several genomic datasets (e.g., gnomAD allele counts and frequencies) into the final filtered outputs. For each variant from \texttt{finalfilt.txt}, it looks up the CN window it falls into by comparing chromosome and position to \texttt{CNwindows.bed}, and appends the CN window information. It then matches the variants with gnomAD allele frequency, count, and number data by comparing chromosome, position, reference, and alternate alleles from \texttt{gnomad\_*} files.
    
\end{enumerate}

\subsection{Hypermutation}

This rule identifies hypermutated samples by calculating the mutation rate and comparing it to a predefined threshold. The rule involves the following steps:

\begin{enumerate}
    \item Tumor Mutation Burden (TMB) calculation. The script extracts relevant columns from the \texttt{vcf\_var\_tot\_dbsnp} file, which contains all variants for the sample, filtered against dbSNP. It applies several filters to keep only variants with certain functional consequences, such as missense mutations or specific intron variants. The script calculates mutation frequencies and coverage statistics for these variants. Variants from the Panel of Normals (PoN) are excluded to avoid considering common sequencing artifacts. The TMB value is calculated as the number of mutations per megabase of sequenced genome.
    \item Microsatellite Instability (MSI) calculation. The script compresses and indexes the\\ \texttt{vcf\_SV\_dbsnp} file. It extracts variants located in known microsatellite regions (from the \texttt{MS\_track} file) and calculates mutation frequencies and coverage statistics for these regions. The script ensures the extracted variants are filtered according to homopolymeric region length and Panel of Normals exclusion. The script \code{microsatellite\_tracks.py} is then used to further analyze the MSI variants. Finally, the percentage of unstable microsatellite regions is calculated.
    \item Old MSI calculation. The rule also calculates an “old” version of MSI by running the process on a subset of the data, using a slightly different method.
\end{enumerate}